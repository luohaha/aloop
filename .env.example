# LLM Provider (anthropic, openai, or gemini)
LLM_PROVIDER=gemini

# API Keys (set the one for your chosen provider)
ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here

# Model (optional, defaults to provider-specific models)
# Anthropic: claude-3-5-sonnet-20241022, claude-3-opus-20240229, etc.
# OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo, etc.
# Gemini: gemini-1.5-pro, gemini-1.5-flash, etc.
MODEL=gemini-2.5-flash

# Base URL (optional, for custom endpoints, proxies, or local deployments)
# Leave empty to use the default API endpoints
ANTHROPIC_BASE_URL=                    # Example: https://api.xiaomimimo.com/anthropic
OPENAI_BASE_URL=                       # Example: https://api.openai-proxy.com/v1
GEMINI_BASE_URL=                       # Example: https://generativelanguage.googleapis.com

# Agent Configuration
MAX_ITERATIONS=100

# Retry Configuration (for handling rate limits and API errors)
RETRY_MAX_ATTEMPTS=5           # Maximum number of retry attempts
RETRY_INITIAL_DELAY=1.0        # Initial delay in seconds
RETRY_MAX_DELAY=60.0           # Maximum delay in seconds

# Memory Management Configuration
MEMORY_MAX_CONTEXT_TOKENS=100000       # Maximum context window size
MEMORY_TARGET_TOKENS=30000             # Target working memory size (soft limit) - LOWERED from 50000
MEMORY_COMPRESSION_THRESHOLD=25000     # Hard limit - compress when exceeded - LOWERED from 40000
MEMORY_SHORT_TERM_SIZE=100             # Number of recent messages to keep
MEMORY_COMPRESSION_RATIO=0.3           # Target compression ratio (0.3 = 30% of original)

# Logging Configuration
LOG_DIR=logs                           # Directory for log files
LOG_LEVEL=DEBUG                        # Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_TO_FILE=true                       # Enable/disable file logging
LOG_TO_CONSOLE=false                   # Enable/disable console logging (WARNING and above)
